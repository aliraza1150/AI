{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58eff882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np # to pre-process data\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox, ttk\n",
    "from PIL import Image, ImageTk\n",
    "import time\n",
    "import threading\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c2dc305",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TqdmProgressCallback(Callback):\n",
    "\n",
    "    def __init__(self, progress, status_label, update_ui_func):\n",
    "        self.progress = progress\n",
    "        self.status_label = status_label\n",
    "        self.update_ui_func = update_ui_func\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epochs = self.params['epochs']\n",
    "        self.progress[\"maximum\"] = self.epochs\n",
    "        self.progress[\"value\"] = 0\n",
    "        self.status_label.config(text=\"Status: Training in progress\")\n",
    "        self.update_ui_func()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.progress[\"value\"] += 1\n",
    "        self.update_ui_func()\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        self.status_label.config(text=\"Status: Training completed\")\n",
    "        self.update_ui_func()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30858080",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class FaceRecognitionApp(tk.Tk):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.title(\"Face Recognition App\")\n",
    "        self.geometry(\"800x600\")\n",
    "        self.configure(bg=\"black\")\n",
    "        \n",
    "        # store the recognition state\n",
    "        self.recognition_active = False\n",
    "        \n",
    "        # Create a Frame widget to hold section content\n",
    "        self.section_content = tk.Frame(self)\n",
    "        self.section_content.pack(fill=tk.BOTH, expand=True)\n",
    "        # Create a StringVar to store the selected section\n",
    "        self.selected_section = tk.StringVar()\n",
    "        self.selected_section.set(\"Data Collection\")\n",
    "        self.current_section = \"Data Collection\"\n",
    "        # Create section buttons\n",
    "        self.section_buttons = []\n",
    "        self.section_widgets = {}\n",
    "\n",
    "        self.sections = [\"Data Collection\", \"Model Training\", \"Face Recognition\",\"Information\", \"Privacy\"]\n",
    "\n",
    "        for section in self.sections:\n",
    "            button = tk.Radiobutton(self, text=section, value=section, \n",
    "                                    variable=self.selected_section, command=self.change_section)\n",
    "            button.pack(side=tk.LEFT, padx=10, pady=10)\n",
    "            self.section_buttons.append(button)\n",
    "\n",
    "        # Initialize the camera\n",
    "        self.cap = cv2.VideoCapture(0)\n",
    "        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        # Create the Data Collection section\n",
    "        self.create_data_collection_section()\n",
    "\n",
    "        # Initialize the first section\n",
    "        self.change_section()\n",
    "        \n",
    "        self.dataset_path = tk.StringVar()\n",
    "        \n",
    "        # Define img_width and img_height attributes\n",
    "        self.img_width = 224 # Replace with the width your model expects\n",
    "        self.img_height = 224 # Replace with the height your model expects\n",
    "        \n",
    "        # Bind the on_closing method to the window close event\n",
    "        self.protocol(\"WM_DELETE_WINDOW\", self.on_closing)\n",
    "        \n",
    "        self.faces_captured = 0\n",
    "        \n",
    "    def create_data_collection_section(self):\n",
    "        # Data Collection section\n",
    "        self.data_collection_frame = ttk.Frame(self.section_content)\n",
    "        self.section_widgets[\"Data Collection\"] = self.data_collection_frame\n",
    "\n",
    "\n",
    "        self.user_id_entry_label = tk.Label(self.data_collection_frame, text=\"User ID:\")\n",
    "        self.user_id_entry_label.pack(pady=5)\n",
    "\n",
    "        self.user_id_entry = tk.Entry(self.data_collection_frame)\n",
    "        self.user_id_entry.pack(pady=5)\n",
    "\n",
    "        self.capture_button = tk.Button(self.data_collection_frame, text=\"Start Capturing Faces\", command=self.start_capturing_faces)\n",
    "        self.capture_button.pack(pady=10)\n",
    "\n",
    "        self.preview_label = tk.Label(self.data_collection_frame, text=\"Preview\")\n",
    "        self.preview_label.pack(pady=10)\n",
    "\n",
    "        self.preview_canvas = tk.Canvas(self.data_collection_frame, width=640, height=480, bg=\"black\")\n",
    "        self.preview_canvas.pack()\n",
    "        \n",
    "    def update_camera_feed(self):\n",
    "        if not hasattr(self, \"cap\") or not self.cap.isOpened():\n",
    "            self.cap = cv2.VideoCapture(0)\n",
    "            self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "        ret, frame = self.cap.read()\n",
    "        if not ret:\n",
    "            messagebox.showerror(\"Error\", \"Failed to read camera feed\")\n",
    "            return\n",
    "\n",
    "        self.current_frame = frame.copy()\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = self.face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "            # Get the face ROI\n",
    "            roi = self.current_frame[y:y+h, x:x+w]\n",
    "        if len(faces) > 0 and self.selected_section.get() == \"Face Recognition\" and self.recognition_active:\n",
    "            predictions = self.recognize_faces(roi)\n",
    "            label, confidence = predictions\n",
    "            label_text = f\"{label}: {confidence:.2f}%\"\n",
    "            cv2.putText(frame, label_text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
    "\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = Image.fromarray(frame)\n",
    "        frame = ImageTk.PhotoImage(frame)\n",
    "        if self.selected_section.get() in [\"Data Collection\"]:\n",
    "            self.preview_canvas.create_image(0, 0, anchor=tk.NW, image=frame)\n",
    "            self.preview_canvas.image = frame\n",
    "        elif self.selected_section.get() in [\"Face Recognition\"]:\n",
    "            self.face_recognition_canvas.create_image(0, 0, anchor=tk.NW, image=frame)\n",
    "            self.face_recognition_canvas.image = frame\n",
    "\n",
    "        self.after(30, self.update_camera_feed)\n",
    "\n",
    "        \n",
    "    def stop_camera_feed(self):\n",
    "        if hasattr(self, \"cap\") and self.cap.isOpened():\n",
    "            self.cap.release()\n",
    "    \n",
    "    def start_capturing_faces(self):\n",
    "        user_id = self.user_id_entry.get().strip()\n",
    "        if not user_id:\n",
    "            messagebox.showerror(\"Error\", \"Please enter a User ID.\")\n",
    "            return\n",
    "\n",
    "        self.user_id = user_id\n",
    "        self.faces_captured = 0\n",
    "        self.capture_face()\n",
    "        \n",
    "    def capture_face(self):\n",
    "        if self.faces_captured < 20:\n",
    "            gray = cv2.cvtColor(self.current_frame, cv2.COLOR_BGR2GRAY)\n",
    "            faces = self.face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "            if len(faces) == 1:\n",
    "                x, y, w, h = faces[0]\n",
    "                face_roi = gray[y:y+h, x:x+w]\n",
    "                self.save_face(face_roi, self.user_id)\n",
    "                self.faces_captured += 1\n",
    "                self.after(500, self.capture_face)\n",
    "            else:\n",
    "                self.after(30, self.capture_face)\n",
    "        else:\n",
    "            messagebox.showinfo(\"Success\", f\"Captured 20 face images for User ID: {self.user_id}\")\n",
    "\n",
    "    def save_face(self, face, user_id):\n",
    "        faces_folder = os.path.join(\"faces\", user_id)\n",
    "        os.makedirs(faces_folder, exist_ok=True)\n",
    "        face_count = len(os.listdir(faces_folder))\n",
    "        face_path = os.path.join(faces_folder, f\"face_{face_count}.jpg\")\n",
    "        cv2.imwrite(face_path, face)\n",
    "\n",
    "    def create_model_training_section(self):\n",
    "        self.model_training_frame = ttk.Frame(self.section_content)\n",
    "        self.section_widgets[\"Model Training\"] = self.model_training_frame\n",
    "        \n",
    "        self.dataset_label = tk.Label(self.model_training_frame, text=\"No dataset selected\")\n",
    "        self.dataset_label.pack(pady=5)\n",
    "        \n",
    "        self.select_dataset_button = tk.Button(self.model_training_frame, text=\"Select Dataset\", command=self.select_dataset)\n",
    "        self.select_dataset_button.pack(pady=5)\n",
    "        \n",
    "        self.model_name_label = tk.Label(self.model_training_frame, text=\"Model Name:\")\n",
    "        self.model_name_label.pack(pady=5)\n",
    "\n",
    "        self.model_name_entry = tk.Entry(self.model_training_frame)\n",
    "        self.model_name_entry.pack(pady=5)\n",
    "\n",
    "        self.model_type_label = tk.Label(self.model_training_frame, text=\"Model Type:\")\n",
    "        self.model_type_label.pack(pady=5)\n",
    "\n",
    "        self.selected_model = tk.StringVar()\n",
    "        self.selected_model.set(\"CNN\")  # set the default value\n",
    "        self.model_type_menu = tk.OptionMenu(self.model_training_frame, self.selected_model, \"CNN\", \"Eigenfaces\", \"LBPH\")\n",
    "        self.model_type_menu.pack(pady=5)\n",
    "\n",
    "        self.train_button = tk.Button(self.model_training_frame, text=\"Start Model Training\", command=self.start_model_training)\n",
    "        self.train_button.pack(pady=10)\n",
    "\n",
    "        self.progress = ttk.Progressbar(self.model_training_frame, orient=tk.HORIZONTAL, length=400, mode=\"determinate\")\n",
    "        self.progress.pack(pady=10)\n",
    "\n",
    "        self.status_label = tk.Label(self.model_training_frame, text=\"Status: Not started\", wraplength=400)\n",
    "        self.status_label.pack(pady=5)\n",
    "        \n",
    "    def select_dataset(self):\n",
    "        self.dataset_path.set(filedialog.askdirectory(initialdir=os.getcwd(), title=\"Select Dataset\"))\n",
    "        self.dataset_label.config(text=f\"Selected dataset: {self.dataset_path.get()}\")\n",
    "\n",
    "            \n",
    "    def preprocess_dataset_cnn(self, dataset_path):\n",
    "        preprocessed_images = []\n",
    "        labels = []\n",
    "        label_dict = {}\n",
    "        label_count = 0\n",
    "\n",
    "        for subdir, dirs, files in os.walk(dataset_path):\n",
    "            if len(files) > 0:\n",
    "                if os.path.basename(subdir) not in label_dict:\n",
    "                    label_dict[os.path.basename(subdir)] = label_count\n",
    "                    label_count += 1\n",
    "\n",
    "                for file in files:\n",
    "                    if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                        # Preprocess the image\n",
    "                        img = cv2.imread(os.path.join(subdir, file))\n",
    "                        img = cv2.resize(img, (224, 224))\n",
    "                        img = img.astype('float32') / 255\n",
    "\n",
    "                        # Add the preprocessed image and its label to the lists\n",
    "                        preprocessed_images.append(img)\n",
    "                        labels.append(label_dict[os.path.basename(subdir)])\n",
    "\n",
    "        preprocessed_images = np.array(preprocessed_images)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        return preprocessed_images, labels\n",
    "\n",
    "    \n",
    "    def train_cnn_model(self, X_train, X_test, y_train, y_test):\n",
    "        # Convert labels to one-hot encoding\n",
    "        num_classes = len(np.unique(y_train))\n",
    "        y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "        y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "        \n",
    "        \n",
    "        # Define the CNN model architecture\n",
    "        model = models.Sequential([\n",
    "            layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "        ])\n",
    "        \n",
    "        # Compile the model\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "                      metrics=['accuracy'])\n",
    "        \n",
    "        # Train the model\n",
    "        progress_callback = TqdmProgressCallback(\n",
    "            self.progress, self.status_label, self.update_ui_during_training\n",
    "        )\n",
    "        \n",
    "        history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), callbacks=[progress_callback], verbose=0)\n",
    "\n",
    "        # Save the trained model\n",
    "        model_name = self.model_name_entry.get().strip()\n",
    "        if not model_name:\n",
    "            model_name = 'cnn_model'\n",
    "        model.save(f\"{model_name}.h5\")\n",
    "\n",
    "        # Display training results\n",
    "        messagebox.showinfo(\"Success\", f\"Model '{model_name}' trained and saved successfully.\")\n",
    "\n",
    "    def preprocess_dataset_eigenfaces(self, dataset_path):\n",
    "        image_size = 96\n",
    "        preprocessed_images = []\n",
    "        labels = []\n",
    "        for user_id in os.listdir(dataset_path):\n",
    "            user_folder = os.path.join(dataset_path, user_id)\n",
    "            for image_name in os.listdir(user_folder):\n",
    "                image_path = os.path.join(user_folder, image_name)\n",
    "                image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "                image = cv2.resize(image, (image_size, image_size))\n",
    "                preprocessed_images.append(image)\n",
    "                labels.append(user_id)\n",
    "        return np.array(preprocessed_images), np.array(labels)\n",
    "\n",
    "    def preprocess_dataset_lbph(self, dataset_path):\n",
    "        preprocessed_images = []\n",
    "        labels = []\n",
    "        for user_id in os.listdir(dataset_path):\n",
    "            user_folder = os.path.join(dataset_path, user_id)\n",
    "            for image_name in os.listdir(user_folder):\n",
    "                image_path = os.path.join(user_folder, image_name)\n",
    "                image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "                preprocessed_images.append(image)\n",
    "                labels.append(user_id)\n",
    "        return preprocessed_images, np.array(labels)\n",
    "\n",
    "    def train_and_save_model(self):\n",
    "        dataset_path = self.dataset_path.get()\n",
    "    \n",
    "        if self.selected_model.get() == \"CNN\":\n",
    "            preprocessed_images, labels = self.preprocess_dataset_cnn(dataset_path)\n",
    "            # Add the implementation of the model training and saving for the CNN model here\n",
    "            X_train, X_test, y_train, y_test = train_test_split(preprocessed_images, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "            self.train_cnn_model(X_train, X_test, y_train, y_test)\n",
    "        elif self.selected_model.get() == \"Eigenfaces\":\n",
    "            preprocessed_images, labels = self.preprocess_dataset_eigenfaces(dataset_path)\n",
    "            # Add the implementation of the model training and saving for the Eigenfaces model here\n",
    "        elif self.selected_model.get() == \"LBPH\":\n",
    "            preprocessed_images, labels = self.preprocess_dataset_lbph(dataset_path)\n",
    "            # Add the implementation of the model training and saving for the LBPH model here\n",
    "        else:\n",
    "            messagebox.showerror(\"Error\", \"Invalid model type selected.\")\n",
    "            \n",
    "    def start_model_training(self):\n",
    "        # 100ms delay for update in GUI for each epoch\n",
    "        self.after(100, self.train_and_save_model)\n",
    "    \n",
    "    def update_ui_during_training(self):\n",
    "        self.update_idletasks()\n",
    "        self.update()\n",
    "        \n",
    "    def update_model_output(self, message):\n",
    "        self.model_output.config(state=tk.NORMAL)\n",
    "        self.model_output.insert(tk.END, message + \"\\n\")\n",
    "        self.model_output.config(state=tk.DISABLED)\n",
    "        self.model_output.see(tk.END)\n",
    "        \n",
    "\n",
    "    def create_face_recognition_section(self):\n",
    "        self.face_recognition_section = ttk.Frame(self.section_content)\n",
    "        self.section_widgets[\"Face Recognition\"] = self.face_recognition_section\n",
    "        \n",
    "        load_model_button = ttk.Button(self.face_recognition_section, text=\"Load Model\", command=self.load_model_file)\n",
    "        load_model_button.pack(padx=10, pady=10)\n",
    "\n",
    "        self.loaded_model_label = tk.Label(self.face_recognition_section, text=\"No model loaded\")\n",
    "        self.loaded_model_label.pack(pady=5)\n",
    "        \n",
    "        self.start_face_recognition_button = ttk.Button(self.face_recognition_section, text=\"Recognise Face\", command=self.start_face_recognition)\n",
    "        self.start_face_recognition_button.pack(padx=10, pady=10)\n",
    "        \n",
    "        # Create a canvas to display the video feed\n",
    "        self.face_recognition_canvas = tk.Canvas(self.face_recognition_section, width=640, height=480, bg=\"black\")\n",
    "        self.face_recognition_canvas.pack()\n",
    "        #start_face_recognition(self)\n",
    "        # Create a label to display the recognized person's name\n",
    "        #self.recognized_person_label = tk.Label(self.face_recognition_section, text=\"\", font=(\"Arial\", 16))\n",
    "        #self.recognized_person_label.pack(pady=10)\n",
    "\n",
    "\n",
    "    def load_model_file(self):\n",
    "        filetypes = [('Keras Model', '*.h5'), ('All files', '*')]\n",
    "        model_path = filedialog.askopenfilename(initialdir=os.getcwd(), title=\"Select Model File\", filetypes=filetypes)\n",
    "\n",
    "        if not model_path:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            self.model = load_model(model_path)\n",
    "            self.loaded_model_label.config(text=f\"Model loaded from {model_path}\")\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Failed to load the model: {str(e)}\")\n",
    "\n",
    "    def preprocess_image(self, frame):\n",
    "        # Preprocess the input frame for the CNN model\n",
    "        frame = cv2.resize(frame, (self.img_width, self.img_height))\n",
    "        frame = img_to_array(frame)\n",
    "        frame = np.expand_dims(frame, axis=0)\n",
    "        frame = frame.astype(\"float\") / 255.0\n",
    "        return frame\n",
    "\n",
    "    def recognize_faces(self, frame):\n",
    "        if not hasattr(self, 'model'):\n",
    "            return []\n",
    "        \n",
    "        processed_frame = self.preprocess_image(frame)\n",
    "        predictions = self.model.predict(processed_frame)\n",
    "        \n",
    "        # Check if the predictions array is not empty\n",
    "        if len(predictions) > 0:\n",
    "            label, confidence = np.argmax(predictions), np.max(predictions) * 100\n",
    "            return label, confidence\n",
    "        else:\n",
    "            return None\n",
    "        return predictions\n",
    "\n",
    "\n",
    "    def start_face_recognition(self):\n",
    "        self.recognition_active = True\n",
    "\n",
    "        \n",
    "    def create_information_section(self):\n",
    "    # Information section\n",
    "        self.information_frame = ttk.Frame(self.section_content)\n",
    "        self.section_widgets[\"Information\"] = self.information_frame\n",
    "        # Information section content\n",
    "        self.information_text = (\"\"\"This project is for educational purposes only, as part of a thesis. \n",
    "    The goal is to develop a face recognition system using AI and OpenCV, along with various recognition methods. \n",
    "    The datasets used for training and testing the model are obtained from images of classmates and family members.\"\"\")\n",
    "        self.information_label = tk.Label(self.information_frame, text=self.information_text, wraplength=760, justify=tk.LEFT)\n",
    "        self.information_label.pack(padx=20, pady=20)\n",
    "\n",
    "    def create_privacy_section(self):\n",
    "    # Privacy widgets\n",
    "        self.privacy_frame = ttk.Frame(self.section_content)\n",
    "        self.section_widgets[\"Privacy\"] = self.privacy_frame\n",
    "        # Privacy section content\n",
    "        self.privacy_text = (\"\"\"The images and videos used in this project will not be shared with anyone. \n",
    "    They are solely used for training and testing the face recognition model. \n",
    "    All data handling and processing adhere to UK privacy regulations. \n",
    "    The project is committed to ensuring the privacy and security of the participants' data.\"\"\")\n",
    "        self.privacy_label = tk.Label(self.privacy_frame, text=self.privacy_text, wraplength=760, justify=tk.LEFT)\n",
    "        self.privacy_label.pack(padx=20, pady=20)\n",
    "    \n",
    "    \n",
    "    def change_section(self):\n",
    "        selected_section = self.selected_section.get()\n",
    "\n",
    "        #for widget in self.section_content.winfo_children():\n",
    "            #widget.pack_forget()\n",
    "         # Hide the current section widget\n",
    "        if hasattr(self, \"current_section\"):\n",
    "            self.section_widgets[self.current_section].pack_forget()\n",
    "        \n",
    "        if selected_section not in self.section_widgets:\n",
    "            if selected_section == \"Data Collection\":\n",
    "                self.create_data_collection_section()\n",
    "            elif selected_section == \"Model Training\":\n",
    "                self.create_model_training_section()\n",
    "            elif selected_section == \"Face Recognition\":\n",
    "                self.create_face_recognition_section()\n",
    "            elif selected_section == \"Information\":\n",
    "                self.create_information_section()\n",
    "            elif selected_section == \"Privacy\":\n",
    "                self.create_privacy_section()\n",
    "      #\"\"\"          \n",
    "        if selected_section == \"Data Collection\":\n",
    "            #self.create_data_collection_section()\n",
    "            self.update_camera_feed()\n",
    "            self.recognition_active = False\n",
    "        elif selected_section == \"Model Training\":\n",
    "            #self.create_model_training_section()\n",
    "            self.recognition_active = False\n",
    "        elif selected_section == \"Face Recognition\":\n",
    "            #self.create_face_recognition_section()\n",
    "            self.update_camera_feed()\n",
    "        elif selected_section == \"Information\":\n",
    "            #self.create_information_section()\n",
    "            self.recognition_active = False\n",
    "        elif selected_section == \"Privacy\":\n",
    "            #self.create_privacy_section()\n",
    "            self.recognition_active = False\n",
    "        else:\n",
    "            self.recognition_active = False\n",
    "    \n",
    "        # Show the selected section widget\n",
    "        self.section_widgets[selected_section].pack(fill=tk.BOTH, expand=True, padx=10, pady=10)\n",
    " \n",
    "        self.current_section = selected_section\n",
    "\n",
    "\n",
    "    \n",
    "    def on_closing(self):\n",
    "        self.cap.release()\n",
    "        self.destroy()\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    app = FaceRecognitionApp()\n",
    "    app.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb41d7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(app.current_section)\n",
    "print(app.recognition_active)\n",
    "print(app.faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af51648",
   "metadata": {},
   "outputs": [],
   "source": [
    "app.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30de505f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning CNN model to recognize face\n",
    "'''This script uses a database of images and creates CNN model on top of it to test\n",
    "   if the given image is recognized correctly or not'''\n",
    "\n",
    "'''####### IMAGE PRE-PROCESSING for TRAINING and TESTING data #######'''\n",
    "\n",
    "# Specifying the folder where images are present\n",
    "TrainingImagePath=\"faces\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "affc2150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'faces'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainingImagePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68d2ccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "673b1c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining pre-processing transformations on raw images of training data\n",
    "# These hyper parameters helps to generate slightly twisted versions\n",
    "# of the original image, which leads to a better model, since it learns\n",
    "# on the good and bad mix of images\n",
    "train_datagen = ImageDataGenerator(\n",
    "        shear_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a4d3b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining pre-processing transformations on raw images of testing data\n",
    "# No transformations are done on the testing images\n",
    "test_datagen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fd6f879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 364 images belonging to 22 classes.\n"
     ]
    }
   ],
   "source": [
    "# Generating the Training Data\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        TrainingImagePath,\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b611903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 364 images belonging to 22 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'aassdd': 0,\n",
       " 'aliraza': 1,\n",
       " 'asasd': 2,\n",
       " 'asda1': 3,\n",
       " 'face1': 4,\n",
       " 'face10': 5,\n",
       " 'face11': 6,\n",
       " 'face12': 7,\n",
       " 'face13': 8,\n",
       " 'face14': 9,\n",
       " 'face15': 10,\n",
       " 'face16': 11,\n",
       " 'face2': 12,\n",
       " 'face3': 13,\n",
       " 'face4': 14,\n",
       " 'face5': 15,\n",
       " 'face6': 16,\n",
       " 'face7': 17,\n",
       " 'face8': 18,\n",
       " 'face9': 19,\n",
       " 'irtaza': 20,\n",
       " 'testfaces1': 21}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating the Testing Data\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        TrainingImagePath,\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "\n",
    "# Printing class labels for each face\n",
    "test_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37e65455",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''############ Creating lookup table for all faces ############'''\n",
    "# class_indices have the numeric tag for each face\n",
    "TrainClasses=training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48c2af5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the face and the numeric tag for future reference\n",
    "ResultMap={}\n",
    "for faceValue,faceName in zip(TrainClasses.values(),TrainClasses.keys()):\n",
    "    ResultMap[faceValue]=faceName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "744972ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the face map for future reference\n",
    "import pickle\n",
    "with open(\"ResultsMap.pkl\", 'wb') as fileWriteStream:\n",
    "    pickle.dump(ResultMap, fileWriteStream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2a10f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of Face and its ID {0: 'aassdd', 1: 'aliraza', 2: 'asasd', 3: 'asda1', 4: 'face1', 5: 'face10', 6: 'face11', 7: 'face12', 8: 'face13', 9: 'face14', 10: 'face15', 11: 'face16', 12: 'face2', 13: 'face3', 14: 'face4', 15: 'face5', 16: 'face6', 17: 'face7', 18: 'face8', 19: 'face9', 20: 'irtaza', 21: 'testfaces1'}\n",
      "\n",
      " The Number of output neurons:  22\n"
     ]
    }
   ],
   "source": [
    "# The model will give answer as a numeric tag\n",
    "# This mapping will help to get the corresponding face name for it\n",
    "print(\"Mapping of Face and its ID\",ResultMap)\n",
    " \n",
    "# The number of neurons for the output layer is equal to the number of faces\n",
    "OutputNeurons=len(ResultMap)\n",
    "print('\\n The Number of output neurons: ', OutputNeurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f69668bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''######################## Create CNN deep learning model ########################'''\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    " \n",
    "'''Initializing the Convolutional Neural Network'''\n",
    "classifier= Sequential()\n",
    " \n",
    "''' STEP--1 Convolution\n",
    "# Adding the first layer of CNN\n",
    "# we are using the format (64,64,3) because we are using TensorFlow backend\n",
    "# It means 3 matrix of size (64X64) pixels representing Red, Green and Blue components of pixels\n",
    "'''\n",
    "classifier.add(Convolution2D(32, kernel_size=(5, 5), strides=(1, 1), input_shape=(64,64,3), activation='relu'))\n",
    " \n",
    "'''# STEP--2 MAX Pooling'''\n",
    "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
    " \n",
    "'''############## ADDITIONAL LAYER of CONVOLUTION for better accuracy #################'''\n",
    "classifier.add(Convolution2D(64, kernel_size=(5, 5), strides=(1, 1), activation='relu'))\n",
    " \n",
    "classifier.add(MaxPool2D(pool_size=(2,2)))\n",
    " \n",
    "'''# STEP--3 FLattening'''\n",
    "classifier.add(Flatten())\n",
    " \n",
    "'''# STEP--4 Fully Connected Neural Network'''\n",
    "classifier.add(Dense(64, activation='relu'))\n",
    " \n",
    "classifier.add(Dense(OutputNeurons, activation='softmax'))\n",
    " \n",
    "'''# Compiling the CNN'''\n",
    "#classifier.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "classifier.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics=[\"accuracy\"])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1320e606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8/8 [==============================] - 13s 211ms/step - loss: 76.8298 - accuracy: 0.0464 - val_loss: 3.3841 - val_accuracy: 0.0594\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 1s 158ms/step - loss: 3.1939 - accuracy: 0.0657 - val_loss: 2.5825 - val_accuracy: 0.2625\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 1s 159ms/step - loss: 2.6305 - accuracy: 0.2646 - val_loss: 2.3551 - val_accuracy: 0.2594\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 1s 164ms/step - loss: 2.2179 - accuracy: 0.3295 - val_loss: 1.6082 - val_accuracy: 0.5063\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 1s 154ms/step - loss: 1.5932 - accuracy: 0.5544 - val_loss: 1.0877 - val_accuracy: 0.6906\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 1s 148ms/step - loss: 1.1150 - accuracy: 0.6676 - val_loss: 1.2097 - val_accuracy: 0.6562\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 1s 168ms/step - loss: 1.7148 - accuracy: 0.5074 - val_loss: 1.2464 - val_accuracy: 0.6906\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 1s 156ms/step - loss: 1.2720 - accuracy: 0.6373 - val_loss: 0.7300 - val_accuracy: 0.7937\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 1s 157ms/step - loss: 0.8754 - accuracy: 0.7165 - val_loss: 0.4010 - val_accuracy: 0.8969\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 1s 154ms/step - loss: 0.5378 - accuracy: 0.8287 - val_loss: 0.3824 - val_accuracy: 0.8875\n",
      "###### Total Time Taken:  0 Minutes ######\n"
     ]
    }
   ],
   "source": [
    "###########################################################\n",
    "import time\n",
    "# Measuring the time taken by the model to train\n",
    "StartTime=time.time()\n",
    " \n",
    "# Starting the model training\n",
    "classifier.fit(\n",
    "                    training_set,\n",
    "                    steps_per_epoch=8,\n",
    "                    epochs=10,\n",
    "                    validation_data=test_set,\n",
    "                    validation_steps=10)\n",
    " \n",
    "EndTime=time.time()\n",
    "print(\"###### Total Time Taken: \", round((EndTime-StartTime)/60), 'Minutes ######')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74ae9b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "Prediction is:  face4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    " \n",
    "ImagePath='Face_Images/Final_Testing_Images/face4/3face4.jpg'\n",
    "test_image=image.load_img(ImagePath,target_size=(64, 64))\n",
    "test_image=image.img_to_array(test_image)\n",
    " \n",
    "test_image=np.expand_dims(test_image,axis=0)\n",
    " \n",
    "result=classifier.predict(test_image,verbose=0)\n",
    "#print(training_set.class_indices)\n",
    " \n",
    "print('####'*10)\n",
    "print('Prediction is: ',ResultMap[np.argmax(result)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b167f0cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
